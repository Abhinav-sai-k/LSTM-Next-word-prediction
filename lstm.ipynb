{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bee0582",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd4bdae4",
   "metadata": {},
   "source": [
    "## 1. DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75474ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import joblib\n",
    "\n",
    "# Load “Paradise Lost” by John Milton\n",
    "data = gutenberg.raw('milton-paradise.txt')\n",
    "\n",
    "joblib.dump(data, 'paradise.txt')   # saving data to a file and reusing it later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93de6d",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2233594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load text\n",
    "txt_file = joblib.load('paradise.pkl')\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([txt_file])\n",
    "total_words = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77016228",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1f799",
   "metadata": {},
   "source": [
    "## 2.1 Creating tokens and sequences (text to num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences (n-grams)\n",
    "input_sequences = []\n",
    "for line in txt_file.split(\"\\n\"):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Padding\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5adca",
   "metadata": {},
   "source": [
    "## 3. splitting Data x,y to train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baae9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "x_train_t = torch.tensor(x_train, dtype=torch.long)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_t  = torch.tensor(x_test, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = TensorDataset(x_train_t, y_train_t)\n",
    "test_ds  = TensorDataset(x_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481146c",
   "metadata": {},
   "source": [
    "## 4. Defining model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class NextWordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim // 2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim // 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout(x[:, -1, :])  # take last time step\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate model\n",
    "model = NextWordLSTM(total_words, embed_dim=100, hidden_dim=150)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcd8d5",
   "metadata": {},
   "source": [
    "## 5. Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2745dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience, trigger = 5, 0  # early stopping\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:  # xb means x batch and yb means y batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger = 0\n",
    "        torch.save(model.state_dict(), \"best_lstm.pth\")\n",
    "    else:\n",
    "        trigger += 1\n",
    "        if trigger>= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7907c6",
   "metadata": {},
   "source": [
    "## 6. Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    model.eval()\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    token_tensor = torch.tensor(token_list, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(token_tensor)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# Example\n",
    "input_text = \"Of man’s first disobedience\"\n",
    "print(f\"Input: {input_text}\")\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Predicted next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a2d00",
   "metadata": {},
   "source": [
    "## 7. Predict top 5 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_top_k_words(model, tokenizer, text, max_sequence_len, k=5):\n",
    "    model.eval()\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    # Trim to match input length\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "\n",
    "    # Pad sequence\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    token_tensor = torch.tensor(token_list, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(token_tensor)\n",
    "        probs = F.softmax(logits, dim=1)  # convert to probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "\n",
    "    # Map indices back to words\n",
    "    top_words = []\n",
    "    for i in range(k):\n",
    "        word = None\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == top_indices[0][i].item():\n",
    "                word = w\n",
    "                break\n",
    "        top_words.append((word, top_probs[0][i].item()))\n",
    "\n",
    "    return top_words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Of man’s first disobedience\"\n",
    "print(f\"Input: {input_text}\")\n",
    "predictions = predict_top_k_words(model, tokenizer, input_text, max_sequence_len, k=5)\n",
    "\n",
    "print(\"\\nTop 5 Predictions:\")\n",
    "for word, prob in predictions:\n",
    "    print(f\"{word} ({prob:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_k_words(model, tokenizer,\"In the beginnings how the heavens and earth rose out of\", max_sequence_len, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b33746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving files\n",
    "\n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')  # saving tokenizer for later use\n",
    "joblib.dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
